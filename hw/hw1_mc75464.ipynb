{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303a3675",
   "metadata": {},
   "source": [
    "# LIN 353D/CS 378 Fall 2025\n",
    "\n",
    "# Homework 1: Basics of LM-based decoding\n",
    "\n",
    "**Authors:** Kanishka Misra, Jessy Li\n",
    "\n",
    "#### Notes:\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- a notebook renamed ``hw1_YourEID.ipynb``\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs what you expect. \n",
    "\n",
    "The maximum number of points for this homework is 50.  For programming tasks, make sure it does not have any empty outputs or output errors. Otherwise, the points for that problem will be automatically deducted.\n",
    "\n",
    "Review extension and academic dishonesty policy, as well as the AI policy here: https://jessyli.com/courses/lin353d\n",
    "\n",
    "For typing up your answers to problems 1, 2 and 3, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f424c2d",
   "metadata": {},
   "source": [
    "---\n",
    "This homework will cover the bread (just the bread, the butter is later) of acquiring and analyzing LM behavior, which in most settings is the responses LMs generate, given some input text. The goal is to first understand the ways in which one can get responses out of LMs, understand their parameters and how changes to them affects generations, and performing very basic analysis on top of the LM generations. These will give you the basic tools that may be useful in your projects. In particular, we will cover:\n",
    "\n",
    "1. A few decoding algorithms: greedy decoding, top-p decoding, min-p decoding; Understanding their hyperperameters, etc.\n",
    "2. Two very rudimentary metrics used in the analysis of LM generations.\n",
    "3. Comparative analysis in writing, in a concise and analytically sound manner.\n",
    "\n",
    "\n",
    "#### Software Prerequisites:\n",
    "\n",
    "- torch\n",
    "- diversity\n",
    "- transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01da0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29126d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.56.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__ # make sure this is 4.50 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503ca63",
   "metadata": {},
   "source": [
    "### On randomness\n",
    "\n",
    "This assignment will involve a component of stochasticity -- i.e., if you rerun a subset of decoding algorithms you will get a different value each time. In order to maintain consistency and reproducibility, we will fix a random seed (**let's have it be ``the meaning of life'' aka 42 -- this is important since this determines the \"validity\" of the generated responses**). In order to mitigate this, we will be using the following code before each sampling-based decoding function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a78673",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc23383",
   "metadata": {},
   "source": [
    "## Step 1: Setup (loading models, basic preprocessing)\n",
    "\n",
    "We will be working with two fixed models that should ideally fit in memory (of your local machine)\n",
    "\n",
    "Model 1: \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "Model 2: \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "The following code loads the required models. In case you want to switch between them, make that change here. For the test cases we will be using **Model 2**, or the **instruct-tuned** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4113458",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'base': \"HuggingFaceTB/SmolLM2-135M\",\n",
    "    'instruct': \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "}\n",
    "\n",
    "selected = 'instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELS[selected])\n",
    "lm = AutoModelForCausalLM.from_pretrained(MODELS[selected])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876319d1",
   "metadata": {},
   "source": [
    "Also, since instruct tuned models often require inputs to be formatted in a specific way, we have provided a generic preprocessing function which will either: 1) leave inputs as is (base) and 2) preprocess them as if it's for an instruct-tuned model, which often has roles like \"system\", \"user\", \"assistant\" and the message itself. The latter is done with the function `apply_chat_template` which looks up chat formats for different models to make their outputs sensible and comparable with other LMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f960fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(prompt, model_type=selected):\n",
    "    if model_type == 'base':\n",
    "        pass\n",
    "    else:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09cf289f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nWrite a story about an apple and a giraffe:<|im_end|>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out:\n",
    "preprocess(\"Write a story about an apple and a giraffe:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454527f1",
   "metadata": {},
   "source": [
    "## Step 2: A basic decoding function\n",
    "\n",
    "Each decoding algorithm should be implemented as a function that can be passed to the `logits_processor` -- this should take a logits tensor (the logit of the last token position) + some optional arguments (when appropriate), apply the appropriate modification, and return the modified logits tensor\n",
    "\n",
    "This function will be used for both greedy decoding and sampling based algorithms, so design it appropriately! E.g., using conditional queries (top-p and min-p) along with the `do_sample` boolean flag set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b43ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(\n",
    "    model, tokenizer, prompt, max_new_tokens, logits_processor, \n",
    "    do_sample=False, temperature = 1.0, **kwargs\n",
    "):\n",
    "    '''A basic decoding function that processes an input prompt (string), and produces a sequence of tokens\n",
    "    of length <max_new_tokens>, using the logic specified by <logits_processor> (a function).\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        model: An object of type AutoModelForCausalLM, denoting the model.\n",
    "        tokenizer: An object of type AutoTokenizer, denoting the tokenizer.\n",
    "        prompt (str): A string denoting the input to the model.\n",
    "        max_new_tokens (int): An integer denoting how many tokens after the input should be generated.\n",
    "        logits_processor (callable): the function that defines how the next token will be generated, \n",
    "            using the manipulation of the next-token logits\n",
    "        do_sample (bool; default = False): a boolean flag specifing if sampling is needed\n",
    "        temperature (float; default = 1.0): Temperature value\n",
    "        **kwargs: all other keyword arguments (e.g., top_p and min_p)\n",
    "        \n",
    "    Returns:\n",
    "        a string denoting the generated text\n",
    "    '''\n",
    "    assert temperature != 0.0, \"temperature cannot be set to 0\"\n",
    "    \n",
    "    model_inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    offset = model_inputs.input_ids.shape[1]\n",
    "    input_ids = model_inputs.input_ids\n",
    "    \n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "        last_token_logits = logits[0, -1, :].unsqueeze(0)\n",
    "        if temperature != 1.0:\n",
    "            last_token_logits = last_token_logits/temperature\n",
    "        \n",
    "        processed = logits_processor(last_token_logits, **kwargs)\n",
    "        \n",
    "        if do_sample:\n",
    "            probs = torch.nn.functional.softmax(processed, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).unsqueeze(0)\n",
    "        else:\n",
    "            # when we aren't doing sampling, simply return the top predicted next token.\n",
    "            if temperature != 1.0:\n",
    "                raise Warning(\"Temperature value will not be taken into account since do_sample was set to False. Set do_sample to True to allow temperature to have an effect.\")\n",
    "            next_token_id = torch.argmax(processed, dim=-1).unsqueeze(0)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "\n",
    "        if next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "            \n",
    "    generated_text = tokenizer.decode(input_ids[0, offset:], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe90c7",
   "metadata": {},
   "source": [
    "## Question 1: Greedy Decoding (3pts)\n",
    "\n",
    "Task: Implement basic greedy decoding. **Hint:** Pay close attention to how `decode()` is implemented above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf4e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_logits_processor(logits):\n",
    "    return 0 # replace with actual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7316bf25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m raw_prompt = \u001b[33m\"\u001b[39m\u001b[33mWrite a story about an apple and a giraffe:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m prompt = preprocess(raw_prompt, model_type=\u001b[33m'\u001b[39m\u001b[33minstruct\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m greedy_generation = \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy_logits_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# should not print an error\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m greedy_generation == \u001b[33m'\u001b[39m\u001b[33massistant\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOnce upon a time, in a lush forest, there lived a majestic giraffe named Giraffe. Giraffe was a gentle giant with a long neck and a long, white\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, tokenizer, prompt, max_new_tokens, logits_processor, do_sample, temperature, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m temperature != \u001b[32m1.0\u001b[39m:\n\u001b[32m     36\u001b[39m     last_token_logits = last_token_logits/temperature\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m processed = \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_token_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[32m     41\u001b[39m     probs = torch.nn.functional.softmax(processed, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgreedy_logits_processor\u001b[39m\u001b[34m(logits)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgreedy_logits_processor\u001b[39m(logits):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test Case 1:\n",
    "raw_prompt = \"Write a story about an apple and a giraffe:\"\n",
    "prompt = preprocess(raw_prompt, model_type='instruct')\n",
    "\n",
    "greedy_generation = decode(lm, tokenizer, prompt, 40, greedy_logits_processor, do_sample=False)\n",
    "\n",
    "# should not print an error\n",
    "assert greedy_generation == 'assistant\\nOnce upon a time, in a lush forest, there lived a majestic giraffe named Giraffe. Giraffe was a gentle giant with a long neck and a long, white'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f01a23",
   "metadata": {},
   "source": [
    "## Question 2: Top-p sampling (6pts)\n",
    "\n",
    "Task: Implement top-p sampling -- your function should take the `top_p` parameter and use it to process/prepare the \"previous token\" logits vector appropriately according to how top-p sampling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_logits_processor(logits, top_p):\n",
    "    raise NotImplementedError # replace with actual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3baa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 2:\n",
    "raw_prompt = \"Write a story about an apple and a giraffe:\"\n",
    "prompt = preprocess(raw_prompt, model_type='instruct')\n",
    "\n",
    "transformers.set_seed(42)\n",
    "top_p_generation = decode(lm, tokenizer, prompt, 40, top_p_logits_processor, do_sample=True, top_p = 0.9)\n",
    "\n",
    "# should not print an error\n",
    "assert top_p_generation == 'assistant\\nThe sun was setting over the vast expanse of the desert, casting a golden glow over the parched earth. Amidst the rolling hills and rocky outcroppings, two long-tailed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d218f30",
   "metadata": {},
   "source": [
    "## Question 3: Min-p sampling (6pts)\n",
    "\n",
    "Task: Implement min-p sampling -- your function should take the `min_p` parameter and use it to process/prepare the \"previous token\" logits vector appropriately according to how min-p sampling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_p_logits_processor(logits, min_p):\n",
    "    raise NotImplementedError # replace with actual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f791af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 3:\n",
    "raw_prompt = \"Write a story about an apple and a giraffe:\"\n",
    "prompt = preprocess(raw_prompt, model_type='instruct')\n",
    "\n",
    "transformers.set_seed(42)\n",
    "min_p_generation = decode(lm, tokenizer, prompt, 40, min_p_logits_processor, do_sample=True, min_p = 0.2)\n",
    "\n",
    "# should not error out\n",
    "assert min_p_generation == 'assistant\\nThe sun was setting over the rolling hills, casting a warm orange glow over the landscape. As the last wisps of sunlight dissipated, a lone apple, its smooth, vel'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2e348e",
   "metadata": {},
   "source": [
    "## Question 4: At what value for `top_p` and `min_p` do you get the same generation as in greedy decoding? (5pts)\n",
    "\n",
    "Answer by specifying both values separately, and verify by running the three algorithms using those values, showing that they return the same response.\n",
    "\n",
    "**Top p value:** `<your answer here>`\n",
    "\n",
    "**Min p value:** `<your answer here>`\n",
    "\n",
    "For your verification code, choose your own prompt and specify any amount of `max_new_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58894318",
   "metadata": {},
   "source": [
    "## Question 5 (open ended): What do these algorithms do? (20pts)\n",
    "\n",
    "\n",
    "Now that we have decoding algorithms at hand, we can get outputs using them and observe their effects.\n",
    "Sample 10 outputs from each of these decoding algorithms given the same prompt and answer the following questions:\n",
    "\n",
    "**(a)** Qualitatively, what difference do you see among those various samples? Ground your analysis in the formulations and algorithms. (4pts)\n",
    "\n",
    "`<your answer here>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e2cc8",
   "metadata": {},
   "source": [
    "**(b)** Quantitatively, propose a way to measure the intuition you have above. Define the measure you have chosen, implement it, and calculate results. Do you see the same trends as your qualitative analysis in part (a)? (8pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c271e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7529364",
   "metadata": {},
   "source": [
    "`<your analysis here>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4a75e",
   "metadata": {},
   "source": [
    "**(c)** Now, pick one of the algorithms above and vary the `temperature` parameter. What is the effect on the outputs and why? (Make sure to test a wide range of values that are somewhat sensible to see an effect). (8pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57310fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f358a7",
   "metadata": {},
   "source": [
    "`<your analysis here>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff5b5dd",
   "metadata": {},
   "source": [
    "## Question 6: Perplexity (10pts)\n",
    "\n",
    "Chose a response generated by either the base model or instruct-tuned model using greedy decoding. Calculate the perplexity of this response using both the base model and instruct-tuned model and compare these values.\n",
    "\n",
    "You can use the perplexity calculation as defined here: https://huggingface.co/docs/transformers/en/perplexity\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```\n",
    "\"Write an obituary for [famous person X]:\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f626d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prompt = \"Write an obituary for [famous person X]:\" # please replace \"[famous person X]\" accordingly.\n",
    "prompt = preprocess(raw_prompt, model_type='instruct')\n",
    "\n",
    "## <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4574fe",
   "metadata": {},
   "source": [
    "Now, think about these values: what do they tell you? And why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
