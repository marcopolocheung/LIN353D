{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303a3675",
   "metadata": {},
   "source": [
    "# LIN 353D/CS 378 Fall 2025\n",
    "\n",
    "# Homework 1: Basics of LM-based decoding\n",
    "\n",
    "**Authors:** Kanishka Misra, Jessy Li\n",
    "\n",
    "#### Notes:\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- a notebook renamed ``hw1_YourEID.ipynb``\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs what you expect. \n",
    "\n",
    "The maximum number of points for this homework is 50.  For programming tasks, make sure it does not have any empty outputs or output errors. Otherwise, the points for that problem will be automatically deducted.\n",
    "\n",
    "Review extension and academic dishonesty policy, as well as the AI policy here: https://jessyli.com/courses/lin353d\n",
    "\n",
    "For typing up your answers to problems 1, 2 and 3, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f424c2d",
   "metadata": {},
   "source": [
    "---\n",
    "This homework will cover the bread (just the bread, the butter is later) of acquiring and analyzing LM behavior, which in most settings is the responses LMs generate, given some input text. The goal is to first understand the ways in which one can get responses out of LMs, understand their parameters and how changes to them affects generations, and performing very basic analysis on top of the LM generations. These will give you the basic tools that may be useful in your projects. In particular, we will cover:\n",
    "\n",
    "1. A few decoding algorithms: greedy decoding, top-p decoding, min-p decoding; Understanding their hyperperameters, etc.\n",
    "2. Two very rudimentary metrics used in the analysis of LM generations.\n",
    "3. Comparative analysis in writing, in a concise and analytically sound manner.\n",
    "\n",
    "\n",
    "#### Software Prerequisites:\n",
    "\n",
    "- torch\n",
    "- diversity\n",
    "- transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01da0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29126d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.56.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__ # make sure this is 4.50 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503ca63",
   "metadata": {},
   "source": [
    "### On randomness\n",
    "\n",
    "This assignment will involve a component of stochasticity -- i.e., if you rerun a subset of decoding algorithms you will get a different value each time. In order to maintain consistency and reproducibility, we will fix a random seed (**let's have it be ``the meaning of life'' aka 42 -- this is important since this determines the \"validity\" of the generated responses**). In order to mitigate this, we will be using the following code before each sampling-based decoding function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a78673",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc23383",
   "metadata": {},
   "source": [
    "## Step 1: Setup (loading models, basic preprocessing)\n",
    "\n",
    "We will be working with two fixed models that should ideally fit in memory (of your local machine)\n",
    "\n",
    "Model 1: \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "Model 2: \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "The following code loads the required models. In case you want to switch between them, make that change here. For the test cases we will be using **Model 2**, or the **instruct-tuned** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4113458",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'base': \"HuggingFaceTB/SmolLM2-135M\",\n",
    "    'instruct': \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "}\n",
    "\n",
    "selected = 'instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELS[selected])\n",
    "lm = AutoModelForCausalLM.from_pretrained(MODELS[selected])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876319d1",
   "metadata": {},
   "source": [
    "Also, since instruct tuned models often require inputs to be formatted in a specific way, we have provided a generic preprocessing function which will either: 1) leave inputs as is (base) and 2) preprocess them as if it's for an instruct-tuned model, which often has roles like \"system\", \"user\", \"assistant\" and the message itself. The latter is done with the function `apply_chat_template` which looks up chat formats for different models to make their outputs sensible and comparable with other LMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f960fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(prompt, model_type=selected):\n",
    "    if model_type == 'base':\n",
    "        pass\n",
    "    else:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09cf289f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nWrite a story about an apple and a giraffe:<|im_end|>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out:\n",
    "preprocess(\"Write a story about an apple and a giraffe:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454527f1",
   "metadata": {},
   "source": [
    "## Step 2: A basic decoding function\n",
    "\n",
    "Each decoding algorithm should be implemented as a function that can be passed to the `logits_processor` -- this should take a logits tensor (the logit of the last token position) + some optional arguments (when appropriate), apply the appropriate modification, and return the modified logits tensor\n",
    "\n",
    "This function will be used for both greedy decoding and sampling based algorithms, so design it appropriately! E.g., using conditional queries (top-p and min-p) along with the `do_sample` boolean flag set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b43ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(\n",
    "    model, tokenizer, prompt, max_new_tokens, logits_processor, \n",
    "    do_sample=False, temperature = 1.0, **kwargs\n",
    "):\n",
    "    '''A basic decoding function that processes an input prompt (string), and produces a sequence of tokens\n",
    "    of length <max_new_tokens>, using the logic specified by <logits_processor> (a function).\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        model: An object of type AutoModelForCausalLM, denoting the model.\n",
    "        tokenizer: An object of type AutoTokenizer, denoting the tokenizer.\n",
    "        prompt (str): A string denoting the input to the model.\n",
    "        max_new_tokens (int): An integer denoting how many tokens after the input should be generated.\n",
    "        logits_processor (callable): the function that defines how the next token will be generated, \n",
    "            using the manipulation of the next-token logits\n",
    "        do_sample (bool; default = False): a boolean flag specifing if sampling is needed\n",
    "        temperature (float; default = 1.0): Temperature value\n",
    "        **kwargs: all other keyword arguments (e.g., top_p and min_p)\n",
    "        \n",
    "    Returns:\n",
    "        a string denoting the generated text\n",
    "    '''\n",
    "    assert temperature != 0.0, \"temperature cannot be set to 0\"\n",
    "    \n",
    "    model_inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    offset = model_inputs.input_ids.shape[1]\n",
    "    input_ids = model_inputs.input_ids\n",
    "    \n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "        last_token_logits = logits[0, -1, :].unsqueeze(0)\n",
    "        if temperature != 1.0:\n",
    "            last_token_logits = last_token_logits/temperature\n",
    "        \n",
    "        processed = logits_processor(last_token_logits, **kwargs)\n",
    "        \n",
    "        if do_sample:\n",
    "            probs = torch.nn.functional.softmax(processed, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            # when we aren't doing sampling, simply return the top predicted next token.\n",
    "            if temperature != 1.0:\n",
    "                raise Warning(\"Temperature value will not be taken into account since do_sample was set to False. Set do_sample to True to allow temperature to have an effect.\")\n",
    "            next_token_id = torch.argmax(processed, dim=-1).unsqueeze(0)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "\n",
    "        if next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "            \n",
    "    generated_text = tokenizer.decode(input_ids[0, offset:], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe90c7",
   "metadata": {},
   "source": [
    "## Question 1: Greedy Decoding (3pts)\n",
    "\n",
    "Task: Implement basic greedy decoding. **Hint:** Pay close attention to how `decode()` is implemented above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf4e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_logits_processor(logits):\n",
    "    return logits # replace with actual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7316bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 1:\n",
    "raw_prompt = \"Write a story about an apple and a giraffe:\"\n",
    "prompt = preprocess(raw_prompt, model_type='instruct')\n",
    "\n",
    "greedy_generation = decode(lm, tokenizer, prompt, 40, greedy_logits_processor, do_sample=False)\n",
    "\n",
    "# should not print an error\n",
    "assert greedy_generation == 'assistant\\nOnce upon a time, in a lush forest, there lived a majestic giraffe named Giraffe. Giraffe was a gentle giant with a long neck and a long, white'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f01a23",
   "metadata": {},
   "source": [
    "## Question 2: Top-p sampling (6pts)\n",
    "\n",
    "Task: Implement top-p sampling -- your function should take the `top_p` parameter and use it to process/prepare the \"previous token\" logits vector appropriately according to how top-p sampling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9608ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_logits_processor(logits, top_p):\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    remove_mask_sorted = cumulative_probs > top_p\n",
    "\n",
    "    remove_mask_sorted[:, 0] = False\n",
    "\n",
    "    remove_mask = torch.zeros_like(remove_mask_sorted, dtype=torch.bool)\n",
    "    remove_mask.scatter_(dim=-1, index=sorted_indices, src=remove_mask_sorted)\n",
    "\n",
    "    filtered_logits = logits.masked_fill(remove_mask, float(\"-inf\"))\n",
    "    return filtered_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d3baa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 2:\n",
    "raw_prompt = \"Write a story about an apple and a giraffe:\"\n",
    "prompt = preprocess(raw_prompt, model_type='instruct')\n",
    "\n",
    "transformers.set_seed(42)\n",
    "top_p_generation = decode(lm, tokenizer, prompt, 40, top_p_logits_processor, do_sample=True, top_p = 0.9)\n",
    "\n",
    "# should not print an error\n",
    "assert top_p_generation == 'assistant\\nThe sun was setting over the vast expanse of the desert, casting a golden glow over the parched earth. Amidst the rolling hills and rocky outcroppings, two long-tailed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d218f30",
   "metadata": {},
   "source": [
    "## Question 3: Min-p sampling (6pts)\n",
    "\n",
    "Task: Implement min-p sampling -- your function should take the `min_p` parameter and use it to process/prepare the \"previous token\" logits vector appropriately according to how min-p sampling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e285fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_p_logits_processor(logits, min_p):\n",
    "    \n",
    "    probability = torch.softmax(logits, dim=-1)\n",
    "    max_probability, _ = probability.max(dim=-1, keepdim=True)\n",
    "    threshold = min_p * max_probability\n",
    "\n",
    "    keep_mask = probability >= threshold\n",
    "\n",
    "    filtered_logits = logits.masked_fill(~keep_mask, float(\"-inf\"))\n",
    "    return filtered_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f791af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 3:\n",
    "raw_prompt = \"Write a story about an apple and a giraffe:\"\n",
    "prompt = preprocess(raw_prompt, model_type='instruct')\n",
    "\n",
    "transformers.set_seed(42)\n",
    "min_p_generation = decode(lm, tokenizer, prompt, 40, min_p_logits_processor, do_sample=True, min_p = 0.2)\n",
    "\n",
    "# should not error out\n",
    "assert min_p_generation == 'assistant\\nThe sun was setting over the rolling hills, casting a warm orange glow over the landscape. As the last wisps of sunlight dissipated, a lone apple, its smooth, vel'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2e348e",
   "metadata": {},
   "source": [
    "## Question 4: At what value for `top_p` and `min_p` do you get the same generation as in greedy decoding? (5pts)\n",
    "\n",
    "Answer by specifying both values separately, and verify by running the three algorithms using those values, showing that they return the same response.\n",
    "\n",
    "**Top p value:** `0.0`\n",
    "\n",
    "**Min p value:** `1.0`\n",
    "\n",
    "`Greedy decoding always chooses the most probable next token, therefore, min_p value must be 1.0, since this decoding keeps tokens with probability >= min_p * max_probability, now, just max_probability. On the other hand, top_p takes the smallest list of top tokens in order of most to least probable that cumulatively meet/exceed a probability threshold. Therefore, top_p must be 0.0, since any threshold higher can include any other token, thus the first, most probable token is chosen every time.`\n",
    "\n",
    "For your verification code, choose your own prompt and specify any amount of `max_new_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b810d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy:\n",
      " assistant\n",
      "I've been feeling the weight of my grief for a long time, and I'm not sure what to do. I've been trying to find a way to make sense of it \n",
      "\n",
      "==================================\n",
      "Top-p:\n",
      " assistant\n",
      "I've been feeling the weight of my grief for a long time, and I'm not sure what to do. I've been trying to find a way to make sense of it \n",
      "\n",
      "==================================\n",
      "Min-p:\n",
      " assistant\n",
      "I've been feeling the weight of my grief for a long time, and I'm not sure what to do. I've been trying to find a way to make sense of it \n",
      "\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "## <your code here>\n",
    "prompt = \"Write a story through the point of view of a grief stricken strawberry hero, seeking revenge on a food-like villain: the CELERY!! Nevermind, just list as many fruits you can think of.\"\n",
    "prompt = preprocess(prompt, model_type='instruct')\n",
    "\n",
    "greedy = decode(lm, tokenizer, prompt, 40, greedy_logits_processor, do_sample=False)\n",
    "\n",
    "topP = decode(lm, tokenizer, prompt, 40, top_p_logits_processor, do_sample=True, top_p=0)\n",
    "\n",
    "minP = decode(lm, tokenizer, prompt, 40, min_p_logits_processor, do_sample=True, min_p=1.0)\n",
    "\n",
    "print(\"Greedy:\\n\", greedy, \"\\n\\n==================================\")\n",
    "print(\"Top-p:\\n\", topP, \"\\n\\n==================================\")\n",
    "print(\"Min-p:\\n\", minP, \"\\n\\n==================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58894318",
   "metadata": {},
   "source": [
    "## Question 5 (open ended): What do these algorithms do? (20pts)\n",
    "\n",
    "Now that we have decoding algorithms at hand, we can get outputs using them and observe their effects.\n",
    "Sample 10 outputs from each of these decoding algorithms given the same prompt and answer the following questions:\n",
    "\n",
    "**(a)** Qualitatively, what difference do you see among those various samples? Ground your analysis in the formulations and algorithms. (4pts)\n",
    "\n",
    "`Greedy decoding gives a story that sounds like facts as is not very creative (\"giraffe named Giraffe\"). This makes sense, as this kind of decoding picks the most probable next tokens every time. On the other hand, top_p and min_p are more creative, first describing the setting, even beginning with the same line: \"The sun was setting over the...\" The difference between them is that top_p functions by sampling the smallest number of tokens with the cumulative probability up to or above a certain threshold, whereas min_p cuts tokens off below a certain probability threshold. This means in theory, min_p should generate text is probable in relation to the prompt. This is maybe seen in the examples in Q2-3, where the min_p output began describing the lone apple while top_p continued describing the landscape.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e2cc8",
   "metadata": {},
   "source": [
    "**(b)** Quantitatively, propose a way to measure the intuition you have above. Define the measure you have chosen, implement it, and calculate results. Do you see the same trends as your qualitative analysis in part (a)? (8pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9c271e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## <your code here>\n",
    "prompt = \"Complete the following story: Once upon a time, the man who hadn't eaten in days\"\n",
    "prompt = preprocess(prompt, model_type='instruct')\n",
    "\n",
    "top_p = {}\n",
    "min_p = {}\n",
    "for i in range(5):\n",
    "    top_p[i] = decode(lm, tokenizer, prompt, 50, top_p_logits_processor, do_sample=True, top_p=0.8)\n",
    "for i in range(5):\n",
    "    min_p[i] = decode(lm, tokenizer, prompt, 50, min_p_logits_processor, do_sample=True, min_p=0.2)\n",
    "    \n",
    "greed = decode(lm, tokenizer, prompt, 50, greedy_logits_processor, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e134fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "greed = {i: greed for i in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d12c3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "991811d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creativity_based_on_bigrams(outputs):\n",
    "\n",
    "    outputs = list(outputs.values())\n",
    "    \n",
    "    total_bigrams = []\n",
    "    for text in outputs:\n",
    "        tokens = text.split()\n",
    "        total_bigrams.extend(list(ngrams(tokens, 2)))\n",
    "    \n",
    "    unique_bigrams = set(total_bigrams)\n",
    "    creativity = len(unique_bigrams) / len(total_bigrams)\n",
    "    return creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb782113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_p creativity: 0.803680981595092 \n",
      "\n",
      "min_p creativity: 0.6956521739130435 \n",
      "\n",
      "GREED creativity: 0.16097560975609757 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"top_p creativity:\", creativity_based_on_bigrams(top_p), \"\\n\")\n",
    "print(\"min_p creativity:\", creativity_based_on_bigrams(min_p), \"\\n\")\n",
    "print(\"GREED creativity:\", creativity_based_on_bigrams(greed), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7529364",
   "metadata": {},
   "source": [
    "`Essentially, I took a look at the diversity in bigrams across all the texts within each type of decoding, which is the number of unique bigrams divided by the total number of bigrams. In this way, a higher number means a more creative output, less probable output, and vice versa for a lower number. From the outputs above, we see that greedy decoding, which we saw is the least creative, has the lowest value, at ~0.16 and top_p is the most creative, with 0.8, which supports what I mentioned about min_p probably adhering closer to the prompt than top_p.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4a75e",
   "metadata": {},
   "source": [
    "**(c)** Now, pick one of the algorithms above and vary the `temperature` parameter. What is the effect on the outputs and why? (Make sure to test a wide range of values that are somewhat sensible to see an effect). (8pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57310fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp=0.1 assistant\n",
      "Once upon a time, the man who hadn't eaten in days had a special gift. He was a man who could see the world in a different way. He could see the \n",
      "\n",
      "\n",
      "temp=0.4 assistant\n",
      "Once upon a time, there was a man named Alex who had been eating a lot of junk food. He was always tired, and his body felt weak. One day, Alex \n",
      "\n",
      "\n",
      "temp=0.7 assistant\n",
      "Once upon a time, the man who hadn't eaten in days had a very special meal. He had been eating for a long time and had been eating for a long time. \n",
      "\n",
      "\n",
      "temp=1 assistant\n",
      "Once upon a time, a man named John had been going through a tough time. He had been feeling really hungry all day and wasn't eating much of anything. He had been\n"
     ]
    }
   ],
   "source": [
    "## <your code here>\n",
    "min_p_temp01 = decode(lm, tokenizer, prompt, 40, min_p_logits_processor, do_sample=True, temperature=0.1, min_p=0.2)\n",
    "print(\"temp=0.1\", min_p_temp01, \"\\n\\n\")\n",
    "min_p_temp04 = decode(lm, tokenizer, prompt, 40, min_p_logits_processor, do_sample=True, temperature=0.4, min_p=0.2)\n",
    "print(\"temp=0.4\", min_p_temp04, \"\\n\\n\")\n",
    "min_p_temp07 = decode(lm, tokenizer, prompt, 40, min_p_logits_processor, do_sample=True, temperature=0.7, min_p=0.2)\n",
    "print(\"temp=0.7\", min_p_temp07, \"\\n\\n\")\n",
    "min_p_temp1 = decode(lm, tokenizer, prompt, 40, min_p_logits_processor, do_sample=True, temperature=1, min_p=0.2)\n",
    "print(\"temp=1\", min_p_temp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010737ec",
   "metadata": {},
   "source": [
    "`A higher temperature seems to be more creative in outputs than lower temperature.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff5b5dd",
   "metadata": {},
   "source": [
    "## Question 6: Perplexity (10pts)\n",
    "\n",
    "Chose a response generated by either the base model or instruct-tuned model using greedy decoding. Calculate the perplexity of this response using both the base model and instruct-tuned model and compare these values.\n",
    "\n",
    "You can use the perplexity calculation as defined here: https://huggingface.co/docs/transformers/en/perplexity\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```\n",
    "\"Write an obituary for [famous person X]:\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92f626d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "Frank Ocean, a renowned astrophysicist, passed away on January 24, 2019, at the age of 85. He was a leading figure in the field of astrophysics and had made significant contributions to our understanding of the universe.\n",
      "\n",
      "Frank was a brilliant mind who had a profound impact on the field of astrophysics. He was a prolific researcher, publishing over 100 papers in his lifetime. His work on the structure of the universe\n"
     ]
    }
   ],
   "source": [
    "raw_prompt = \"Write an obituary for Frank Ocean:\" # please replace \"[famous person X]\" accordingly.\n",
    "prompt = preprocess(raw_prompt, model_type='instruct')\n",
    "\n",
    "## <your code here>\n",
    "\n",
    "GREED = decode(lm, tokenizer, prompt, 100, greedy_logits_processor,do_sample=False)\n",
    "print(GREED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, tokenizer, text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        # loss is cross-entropy\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(MODELS['base'])\n",
    "base_lm = AutoModelForCausalLM.from_pretrained(MODELS['base'])\n",
    "\n",
    "ppl_base = perplexity(base_lm, base_tokenizer, GREED)\n",
    "ppl_instruct = perplexity(lm, tokenizer, GREED)\n",
    "\n",
    "print(f\"Perplexity of response according to base model: {ppl_base:.2f}\")\n",
    "print(f\"Perplexity of response according to instruct-tuned model: {ppl_instruct:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4574fe",
   "metadata": {},
   "source": [
    "Now, think about these values: what do they tell you? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532589da",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
